# ðŸ“Š Evaluation Metrics Explained (with Formulas & Examples)

When evaluating an object detection or classification model, itâ€™s crucial to understand metrics like **Precision**, **Recall**, **Accuracy**, and **F1 Score**. Here's a simple breakdown:

---

## ðŸŽ¯ 1. Precision â€“ *How many predicted positives were actually correct?*

> â€œOut of all the objects you predicted as positive, how many are truly positive?â€

**Formula:**
```ini
Precision = TP / (TP + FP)
```

**Example:**

- Model predicted 10 objects as positive  
- Actual positives among those: 7  
- â†’ **Precision = 7 / 10 = 0.70 (or 70%)**

---

## ðŸ“¥ 2. Recall â€“ *How many actual positives were correctly predicted?*

> â€œOut of all the actual positive objects, how many did you find?â€

**Formula:**
```ini
Recall = TP / (TP + FN)
```

**Example:**

- Total actual positive objects = 10  
- Model correctly predicted 7 of them  
- â†’ **Recall = 7 / 10 = 0.70 (or 70%)**

---

## ðŸ“Œ 3. Accuracy â€“ *How many predictions were correct overall?*

> â€œOut of all predictions, how many are right?â€

**Formula:**
```ini
Accuracy = (TP + TN) / (TP + FP + FN + TN)
```

**Example:**

- TP = 7, FP = 3, FN = 3, TN = 87  
- â†’ **Accuracy = (7 + 87) / (7 + 3 + 3 + 87) = 94 / 100 = 94%**

---

## âš–ï¸ 4. F1 Score â€“ *The harmonic mean of precision and recall*

> â€œBalance between precision and recall. High only when both are high.â€

**Formula:**
```sql
F1 Score = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```

**Example:**

- Precision = 0.70  
- Recall = 0.70  
- â†’ **F1 Score = 2 Ã— (0.7 Ã— 0.7) / (0.7 + 0.7) = 0.70**

---

## ðŸ“‹ Summary Table

| Metric     | Formula                               | Description                              |
|------------|----------------------------------------|------------------------------------------|
| Precision  | `TP / (TP + FP)`                      | How accurate are positive predictions    |
| Recall     | `TP / (TP + FN)`                      | How well the model finds actual positives|
| F1 Score   | `2 Ã— (P Ã— R) / (P + R)`               | Balance of Precision & Recall            |
| Accuracy   | `(TP + TN) / (TP + FP + FN + TN)`     | Overall correctness                      |

---

## ðŸ” So, when is a model good?

| Metric     | Ideal Value (Rule of Thumb)          |
|------------|---------------------------------------|
| Precision  | â‰¥ 0.80 (80%) â€“ good; > 0.90 â€“ excellent |
| Recall     | â‰¥ 0.80 â€“ good; > 0.90 â€“ excellent       |
| F1 Score   | â‰¥ 0.80 â€“ good; > 0.90 â€“ excellent       |
| Accuracy   | â‰¥ 0.90 â€“ good, *only if* data is balanced |

---

## âœ… Final Takeaway

- A **good model** has **high precision, recall, F1 score, and accuracy**.
- A **bad model** might have **high accuracy** but **low precision/recall** (especially on imbalanced data).
- Always look at **F1 score** for the most reliable summary of model performance.

